{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:20.370489Z","iopub.status.busy":"2021-09-26T07:03:20.369849Z","iopub.status.idle":"2021-09-26T07:03:27.274892Z","shell.execute_reply":"2021-09-26T07:03:27.273816Z","shell.execute_reply.started":"2021-09-26T07:03:20.370436Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import unicodedata\n","import re\n","import nltk\n","nltk.download()\n","nltk.download('stopwords')   \n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","from textblob import TextBlob\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","\n","#model\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score\n","from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import classification_report, confusion_matrix, plot_roc_curve, accuracy_score\n","\n","from imblearn.pipeline import Pipeline\n","from imblearn.over_sampling import SMOTE \n","\n","#plots\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","import matplotlib as mpl\n","from plotly.offline import init_notebook_mode, iplot\n","init_notebook_mode(connected=True)\n","\n","import cufflinks as cf\n","cf.go_offline()\n","cf.set_config_file(offline=False, world_readable=True)\n","import pickle\n","import seaborn as sns\n","#loading small corpus\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","#get data\n","df = pd.read_csv(\"CyberPhish-Dataset.csv\")\n","print(\"Setup Done!\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Explore Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:27.277332Z","iopub.status.busy":"2021-09-26T07:03:27.276995Z","iopub.status.idle":"2021-09-26T07:03:27.306946Z","shell.execute_reply":"2021-09-26T07:03:27.305537Z","shell.execute_reply.started":"2021-09-26T07:03:27.277300Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:27.326164Z","iopub.status.busy":"2021-09-26T07:03:27.325863Z","iopub.status.idle":"2021-09-26T07:03:27.346287Z","shell.execute_reply":"2021-09-26T07:03:27.345349Z","shell.execute_reply.started":"2021-09-26T07:03:27.326134Z"},"trusted":true},"outputs":[],"source":["df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:27.347721Z","iopub.status.busy":"2021-09-26T07:03:27.347381Z","iopub.status.idle":"2021-09-26T07:03:27.376115Z","shell.execute_reply":"2021-09-26T07:03:27.374876Z","shell.execute_reply.started":"2021-09-26T07:03:27.347686Z"},"trusted":true},"outputs":[],"source":["df = df.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:27.379261Z","iopub.status.busy":"2021-09-26T07:03:27.378642Z","iopub.status.idle":"2021-09-26T07:03:27.389184Z","shell.execute_reply":"2021-09-26T07:03:27.388325Z","shell.execute_reply.started":"2021-09-26T07:03:27.379224Z"},"trusted":true},"outputs":[],"source":["df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:27.392367Z","iopub.status.busy":"2021-09-26T07:03:27.391658Z","iopub.status.idle":"2021-09-26T07:03:27.413543Z","shell.execute_reply":"2021-09-26T07:03:27.412711Z","shell.execute_reply.started":"2021-09-26T07:03:27.392334Z"},"trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:27.415283Z","iopub.status.busy":"2021-09-26T07:03:27.414797Z","iopub.status.idle":"2021-09-26T07:03:27.422888Z","shell.execute_reply":"2021-09-26T07:03:27.422052Z","shell.execute_reply.started":"2021-09-26T07:03:27.415251Z"},"trusted":true},"outputs":[],"source":["df[\"label\"].value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Pre-process"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:27.309175Z","iopub.status.busy":"2021-09-26T07:03:27.308876Z","iopub.status.idle":"2021-09-26T07:03:27.324445Z","shell.execute_reply":"2021-09-26T07:03:27.322955Z","shell.execute_reply.started":"2021-09-26T07:03:27.309147Z"},"trusted":true},"outputs":[],"source":["#list of contractions and their related expansions (from web)\n","contractions = { \n","\"ain't\": \"am not\",\n","\"aren't\": \"are not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he will\",\n","\"he'll've\": \"he will have\",\n","\"he's\": \"he is\",\n","\"how'd\": \"how did\",\n","\"how'd'y\": \"how do you\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how does\",\n","\"i'd\": \"i would\",\n","\"i'd've\": \"i would have\",\n","\"i'll\": \"i will\",\n","\"i'll've\": \"i will have\",\n","\"i'm\": \"i am\",\n","\"i've\": \"i have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it would\",\n","\"'ll\": \"will\",\n","\"'ve\": \"have\",\n","\"it'd've\": \"it would have\",\n","\"it'll\": \"it will\",\n","\"it'll've\": \"it will have\",\n","\"it's\": \"it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"mightn't've\": \"might not have\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"mustn't've\": \"must not have\",\n","\"needn't\": \"need not\",\n","\"needn't've\": \"need not have\",\n","\"o'clock\": \"of the clock\",\n","\"oughtn't\": \"ought not\",\n","\"oughtn't've\": \"ought not have\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"shan't've\": \"shall not have\",\n","\"she'd\": \"she would\",\n","\"she'd've\": \"she would have\",\n","\"she'll\": \"she will\",\n","\"she'll've\": \"she will have\",\n","\"she's\": \"she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"shouldn't've\": \"should not have\",\n","\"so've\": \"so have\",\n","\"so's\": \"so is\",\n","\"that'd\": \"that would\",\n","\"that'd've\": \"that would have\",\n","\"that's\": \"that is\",\n","\"there'd\": \"there would\",\n","\"there'd've\": \"there would have\",\n","\"there's\": \"there is\",\n","\"they'd\": \"they would\",\n","\"they'd've\": \"they would have\",\n","\"they'll\": \"they will\",\n","\"they'll've\": \"they will have\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"to've\": \"to have\",\n","\"wasn't\": \"was not\",\n","\" u \": \" you \",\n","\" ur \": \" your \",\n","\" n \": \" and \",\n","\"tbh\":\"to be honest\" }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:32.976328Z","iopub.status.busy":"2021-09-26T07:03:32.975876Z","iopub.status.idle":"2021-09-26T07:03:32.991612Z","shell.execute_reply":"2021-09-26T07:03:32.990551Z","shell.execute_reply.started":"2021-09-26T07:03:32.976285Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:32.993739Z","iopub.status.busy":"2021-09-26T07:03:32.993312Z","iopub.status.idle":"2021-09-26T07:03:33.009836Z","shell.execute_reply":"2021-09-26T07:03:33.008179Z","shell.execute_reply.started":"2021-09-26T07:03:32.993706Z"},"trusted":true},"outputs":[],"source":["def expand(x):\n","    \"\"\"Some of the words like 'i'll', are expanded to 'i will' for better text processing\n","    The list of contractions is taken from the internet\n","    \n","    param x(str): the sentence in which contractions are to be found and expansions are to be done\n","    \n","    return x(str): the expanded sentence\"\"\"\n","    if type(x)== str:\n","        for key in contractions:\n","            value = contractions[key]\n","            x = x.replace(key,value)\n","        return x\n","    else:\n","        return x\n","\n","def remove_accented_chars(x):\n","    \"\"\"The function changes the accented characters into their equivalent normal form,\n","    to do so, normalize function with 'NFKD' is used which replaces the compatibility characters into\n","    theri euivalent\n","    \n","    param x(str): the sentence in which accented characters are to be detected and removes\n","    return x(str): sentence with accented characters replaced by their equivalent\"\"\"\n","    \n","    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return x\n","\n","\n","def make_to_base(x):\n","    \"\"\"Converting the words to their base word and dictionary head word i.e to lemmatize\n","    param x(str): the sentence in which the words are to be converted (lemmatization)\n","    return x(str): the lemmatized sentence\"\"\"\n","    \n","    x_list = []\n","    doc = nlp(x)\n","    \n","    for token in doc:\n","        lemma = str(token.lemma_)\n","        \n","        #in spacy, words like I, you are lemmatized as -PRON- and are,and etc are lemmatized to be,\n","        #since these words are present widely, we keep them as the original. \n","        #Anyways the words will be removed as stop words later\n","        \n","        if lemma == '-PRON-' or lemma == 'be':\n","            lemma = token.text\n","        x_list.append(lemma)\n","    return (\" \".join(x_list)) \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:33.011811Z","iopub.status.busy":"2021-09-26T07:03:33.011500Z","iopub.status.idle":"2021-09-26T07:03:33.027950Z","shell.execute_reply":"2021-09-26T07:03:33.026408Z","shell.execute_reply.started":"2021-09-26T07:03:33.011782Z"},"trusted":true},"outputs":[],"source":["def preprocess(df,d):\n","    \"\"\"Preprocesses the given document by applying the following functionalities\n","    lower: lowers all the characters for uniformity\n","    expansion: expands words like i'll to i will for better text classification\n","    remove special characters: using regex, removes all the punctuations etc\n","    remove space: removes trailing spaces and extra spaces between words\n","    remove accented characters: change accented characters to its normal equivalent\n","    remove stop words: removes the stop words in the sentence\n","    lemmatization: changes the words to their base form\"\"\"\n","    df[d] = df[d].apply(lambda x: x.lower())\n","    df[d] = df[d].apply(expand)\n","    df[d] = df[d].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))\n","    df[d] = df[d].apply(lambda x: \" \".join(x.split()))\n","    df[d] = df[d].apply(lambda x: remove_accented_chars(x))\n","    df[d] = df[d].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:03:33.029907Z","iopub.status.busy":"2021-09-26T07:03:33.029579Z","iopub.status.idle":"2021-09-26T07:05:37.704121Z","shell.execute_reply":"2021-09-26T07:05:37.702637Z","shell.execute_reply.started":"2021-09-26T07:03:33.029876Z"},"trusted":true},"outputs":[],"source":["#calling the function\n","preprocess(df,'email')\n","print(\"Pre processing done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Visualize"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:05:37.706611Z","iopub.status.busy":"2021-09-26T07:05:37.706138Z","iopub.status.idle":"2021-09-26T07:05:37.722998Z","shell.execute_reply":"2021-09-26T07:05:37.721735Z","shell.execute_reply.started":"2021-09-26T07:05:37.706554Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:05:41.644352Z","iopub.status.busy":"2021-09-26T07:05:41.643844Z","iopub.status.idle":"2021-09-26T07:05:43.716474Z","shell.execute_reply":"2021-09-26T07:05:43.715744Z","shell.execute_reply.started":"2021-09-26T07:05:41.644320Z"},"trusted":true},"outputs":[],"source":["fig = plt.figure(figsize=(20,8))\n","#  phishy word cloud\n","text = ' '.join(df.loc[df['label']==1,'email'].values)\n","wc = WordCloud(width=1000, \n","                   height=1000, \n","                   random_state=1, \n","                   background_color='white',\n","                   colormap='Set2',\n","                   collocations=False).generate(text)\n","\n","plt.imshow(wc)\n","plt.axis(\"off\");\n","sr = sorted(wc.words_.items(), key=lambda x:x[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:05:43.718160Z","iopub.status.busy":"2021-09-26T07:05:43.717684Z","iopub.status.idle":"2021-09-26T07:05:46.139215Z","shell.execute_reply":"2021-09-26T07:05:46.135483Z","shell.execute_reply.started":"2021-09-26T07:05:43.718128Z"},"trusted":true},"outputs":[],"source":["fig = plt.figure(figsize=(20,8))\n","#  non-phishy word cloud\n","text = ' '.join(df.loc[df['label']==0,'email'].values)\n","wc = WordCloud(width=1000, \n","                   height=1000, \n","                   random_state=1, \n","                   background_color='white',\n","                   colormap='Set2',\n","                   collocations=False).generate(text)\n","\n","plt.imshow(wc)\n","plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:05:46.453228Z","iopub.status.busy":"2021-09-26T07:05:46.452849Z","iopub.status.idle":"2021-09-26T07:05:46.600805Z","shell.execute_reply":"2021-09-26T07:05:46.599465Z","shell.execute_reply.started":"2021-09-26T07:05:46.453183Z"},"trusted":true},"outputs":[],"source":["sns.countplot(x ='label', data = df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_classification_report(y_test, y_test_pred, title='Classification Report', figsize=(5,3), dpi=100, save_fig_path=None, **kwargs):\n","    \"\"\"\n","    Plot the classification report of sklearn\n","    \n","    Parameters\n","    ----------\n","    y_test : pandas.Series of shape (n_samples,)\n","        Targets.\n","    y_pred : pandas.Series of shape (n_samples,)\n","        Predictions.\n","    title : str, default = 'Classification Report'\n","        Plot title.\n","    fig_size : tuple, default = (8, 6)\n","        Size (inches) of the plot.\n","    dpi : int, default = 70\n","        Image DPI.\n","    save_fig_path : str, defaut=None\n","        Full path where to save the plot. Will generate the folders if they don't exist already.\n","    **kwargs : attributes of classification_report class of sklearn\n","    \n","    Returns\n","    -------\n","        fig : Matplotlib.pyplot.Figure\n","            Figure from matplotlib\n","        ax : Matplotlib.pyplot.Axe\n","            Axe object from matplotlib\n","    \"\"\"    \n","    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n","        \n","    clf_report = classification_report(y_test, y_test_pred, output_dict=True, **kwargs)\n","    keys_to_plot = [key for key in clf_report.keys() if key not in ('accuracy', 'macro avg', 'weighted avg')]\n","    df = pd.DataFrame(clf_report, columns=keys_to_plot).T\n","    #the following line ensures that dataframe are sorted from the majority classes to the minority classes\n","    df.sort_values(by=['support'], inplace=True) \n","    \n","    #first, let's plot the heatmap by masking the 'support' column\n","    rows, cols = df.shape\n","    mask = np.zeros(df.shape)\n","    mask[:,cols-1] = True\n"," \n","    ax = sns.heatmap(df, mask=mask, annot=True, cmap=plt.cm.Purples,\n","            # vmin=0.0,\n","            # vmax=1.0,\n","            linewidths=2, linecolor='white'\n","                    )\n","    \n","    #then, let's add the support column by normalizing the colors in this column\n","    mask = np.zeros(df.shape)\n","    mask[:,:cols-1] = True    \n","    \n","    ax = sns.heatmap(df, mask=mask, annot=True, cmap=plt.cm.Purples, cbar=False,\n","            linewidths=2, linecolor='white', fmt='.0f',\n","            # vmin=df['support'].min(),\n","            # vmax=df['support'].sum(),         \n","            \n","                    ) \n","    # //norm=mpl.colors.Normalize(vmin=df['support'].min(),\n","    #                                   vmax=df['support'].sum()\n","            \n","    plt.title(title)\n","    plt.xticks(rotation = 45)\n","    plt.yticks(rotation = 360)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_confusion_matrix(y_test, y_test_pred, title='Confusion Matrix', figsize=(5,3), dpi=100, save_fig_path=None, **kwargs):\n","    cf_matrix=confusion_matrix(y_test, y_test_pred)\n","    plt.figure(figsize=(4,2), dpi=100)\n","\n","    ax = sns.heatmap(cf_matrix, annot=True, fmt='d',cmap=plt.cm.Purples_r)\n","    ax.set_xlabel(\"Prediction\", fontsize=10, labelpad=10)\n","    ax.xaxis.set_ticklabels(['legitmate', 'phishing'])\n","    ax.set_ylabel(\"Actual\", fontsize=10, labelpad=10)\n","    ax.yaxis.set_ticklabels(['legitmate', 'phishing'])\n","    ax.set_title(\"Confusion Matrix \", fontsize=10, pad=10)\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Train & Test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Vectorization parameters\n","# Range (inclusive) of n-gram sizes for tokenizing text.\n","NGRAM_RANGE = (1, 2)\n","\n","# Limit on the number of features. We use the top 20K features.\n","TOP_K = 20000\n","\n","# Whether text should be split into word or character n-grams.\n","# One of 'word', 'char'.\n","TOKEN_MODE = 'word'\n","\n","# Minimum document/corpus frequency below which a token will be discarded.\n","MIN_DOCUMENT_FREQUENCY = 10\n","MAX_DOCUMENT_FREQUENCY= 4000\n","# Limit on the length of text sequences. Sequences longer than this\n","# will be truncated.\n","MAX_SEQUENCE_LENGTH = 500\n","stopWords =STOP_WORDS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:05:46.686666Z","iopub.status.busy":"2021-09-26T07:05:46.686231Z","iopub.status.idle":"2021-09-26T07:05:46.700241Z","shell.execute_reply":"2021-09-26T07:05:46.699041Z","shell.execute_reply.started":"2021-09-26T07:05:46.686624Z"},"trusted":true},"outputs":[],"source":["#splitting the data\n","X = df[\"email\"].values\n","y = df[\"label\"].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=345,test_size=0.3, stratify=y)\n","kwargs = {\n","            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n","            'dtype': 'int32',\n","            'strip_accents': 'unicode',\n","            'decode_error': 'replace',\n","            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n","            'min_df': MIN_DOCUMENT_FREQUENCY,\n","            'max_df': MAX_DOCUMENT_FREQUENCY,\n","            'stop_words': stopWords,  \n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define a pipeline combining a text feature extractor with multi label classifier\n","NB_pipeline = Pipeline([\n","                ('tfidf', TfidfVectorizer(**kwargs)),\n","                ('smote', SMOTE(random_state=12)),\n","                ('clf', MultinomialNB()),\n","            ])\n","NB_pipeline.fit(X_train, y_train)\n","y_test_pred = NB_pipeline.predict(X_test)\n","accuracy = accuracy_score(y_test,y_test_pred)*100\n","print('Accuracy of NB:', accuracy)\n","plot_confusion_matrix(y_test, y_test_pred,title='Confusion matrix NB',\n","                    figsize=(4, 2), dpi=100,\n","                    target_names=[\"Phishing\",\"Legitmate\"], )\n","\n","plot_classification_report(y_test, y_test_pred, \n","                    title='Classification Report NB',\n","                    figsize=(4, 2), dpi=100,\n","                    target_names=[\"Phishing\",\"Legitmate\"], \n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define a pipeline combining a text feature extractor with multi lable classifier\n","SVM_pipeline = Pipeline([\n","                ('tfidf', TfidfVectorizer(**kwargs)),\n","                ('smote', SMOTE(random_state=12)),\n","                ('clf', LinearSVC(C=1)),\n","            ])\n","SVM_pipeline.fit(X_train, y_train)\n","y_test_pred = SVM_pipeline.predict(X_test)\n","accuracy = accuracy_score(y_test,y_test_pred)*100\n","print('Accuracy of SVM:', accuracy)\n","plot_confusion_matrix(y_test, y_test_pred,title='Confusion matrix SVM',\n","                    figsize=(4, 2), dpi=100,\n","                    target_names=[\"Phishing\",\"Legitmate\"], )\n","plot_classification_report(y_test, y_test_pred, \n","                    title='Classification Report SVM',\n","                    figsize=(4, 2), dpi=100,\n","                    target_names=[\"Phishing\",\"Legitmate\"], \n","                    )\n","pickle.dump(TfidfVectorizer(**kwargs),open('vectorizer.pkl','wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define a pipeline combining a text feature extractor with multi lable classifier\n","RF_pipeline = Pipeline([\n","                ('tfidf', TfidfVectorizer(**kwargs)),\n","                ('smote', SMOTE(random_state=12)),\n","                ('clf', RandomForestClassifier()),\n","            ])\n","RF_pipeline.fit(X_train, y_train)\n","y_test_pred = RF_pipeline.predict(X_test)\n","accuracy = accuracy_score(y_test,y_test_pred)*100\n","print('Accuracy of RF:', accuracy)\n","\n","plot_confusion_matrix(y_test, y_test_pred,title='Confusion matrix RF',\n","                    figsize=(4, 2), dpi=100,\n","                    target_names=[\"Phishing\",\"Legitmate\"], )\n","\n","plot_classification_report(y_test, y_test_pred, \n","                    title='Classification Report RF',\n","                    figsize=(4, 2), dpi=100,\n","                    target_names=[\"Phishing\",\"Legitmate\"], \n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-09-26T07:09:16.400789Z","iopub.status.busy":"2021-09-26T07:09:16.400333Z","iopub.status.idle":"2021-09-26T07:09:17.106928Z","shell.execute_reply":"2021-09-26T07:09:17.105898Z","shell.execute_reply.started":"2021-09-26T07:09:16.400746Z"},"trusted":true},"outputs":[],"source":["pickle.dump(NB_pipeline,open('/Users/leen/Desktop/New model/pkl models/spam-classification/NBmodel-NS.pkl','wb'))\n","pickle.dump(SVM_pipeline,open('/Users/leen/Desktop/New model/pkl models/spam-classification/SVMmodel-NS.pkl','wb'))\n","pickle.dump(RF_pipeline,open('/Users/leen/Desktop/New model/pkl models/spam-classification/RFmodel-NS.pkl','wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_roc_curve(NB_pipeline,X_test,y_test)\n","plot_roc_curve(SVM_pipeline,X_test,y_test)\n","plot_roc_curve(RF_pipeline,X_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kfold = KFold(n_splits=10,shuffle=True)\n","print(\"Accuracy using Cross Validation is :\",np.mean(cross_val_score(NB_pipeline, X_test,y_test,cv=kfold,scoring=\"accuracy\"))*100,\" %\")\n","print(\"Accuracy using Cross Validation is :\",np.mean(cross_val_score(SVM_pipeline, X_test,y_test,cv=kfold,scoring=\"accuracy\"))*100,\" %\")\n","print(\"Accuracy using Cross Validation is :\",np.mean(cross_val_score(RF_pipeline, X_test,y_test,cv=kfold,scoring=\"accuracy\"))*100,\" %\")\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":4}
